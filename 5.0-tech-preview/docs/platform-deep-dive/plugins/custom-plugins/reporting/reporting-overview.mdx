---
title: Reporting plugin
description: The FlowX.AI Reporting plugin helps you build and bootstrap custom reports using data from your BPMN processes. Moreover, it supports technical reports based on process instance data. Integrated with the FlowX.AI Engine, this plugin transforms raw data into actionable insights, enhancing decision-making and optimizing business processes.
---

### Quick walkthrough video

Watch this quick walkthrough video to get started:

<Frame>
<video
  controls
  className="w-full aspect-video"
  src="https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/Reporting%20%281%29.mp4"
></video>
</Frame>

### Data exploration and visualization

The plugin uses **Superset** as a free data exploration and visualization tool.

<Tip>
You can however use your own BI tool like Tableau, PowerBI etc. as an alternative to Superset. 
</Tip>

Use the suggested query structure and logic to ensure accurate reporting and avoid duplicates, even during database updates. Do not just use select on single reporting tables.



![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/reporting.png)


Apache Superset is an open-source software application for data exploration and data visualization able to handle data at a large scale. It enables users to connect their company's databases and perform data analysis, and easily build charts and assemble dashboards.

Superset is also an SQL IDE, so users can write SQL, join data create datasets and so on.

<Card title="Superset documentation" href="https://superset.apache.org/docs/intro/" icon="book"/>


### Plugin architecture

Here’s an overview of the reporting plugin architecture:


![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/Reporting_diagram_small2.png)

### Setting up reporting for a process definition

<Steps>
<Step title="Establish reporting data model to be extracted from process">


If you want to extract custom business parameters for a process, first you should set up the master reporting data model in the published version of that process. This is done in Designer, as explained below in the [**Reporting Data Model**](#reporting-data-model). 

The master reporting data model that is applied to all historical versions is the one belonging to the version that is set as Published. If the currently published version is read-only, you might need to create a new version, create the data model in it, mark it for reporting and set it as published (as explained in the section below).

See [**Reporting Data Model**](#reporting-data-model) section for more details.

</Step>
<Step title="Enable reporting on the process versions to be reported">


This is accomplished by checking the “Use process in reporting” button in the settings / general page of each process version you want to report. Please note that you will not be able to change that setting on read-only versions.


<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/use_proc_reporting.gif)
</Frame>

It is essential that the currently Published version has this setting enabled, as it is the master version for reporting all historical data.

Only the process instances belonging to reporting-enabled versions will be extracted to the database, using the master data model from the currently Published version.

<Info>
The reason why you should first set up the data model on the published (or to-be-published) version is that all changes in the data model of the published version that is also marked “use_in_reporting” are instantly sent to the reporting plugin, potentially causing all historical process instances to be re-computed multiple times before you are done setting up the data model. 
</Info>

To optimize operations, you should always aim to finish the modifications on the master data model on a version that is either not published, or not marked as "use_in_reporting", before marking it as "use_in_reporting and ensuring it is published.

See [**Enabling Process Reporting**](#enabling-process-reporting) section for more details.

</Step>
</Steps>


## Reporting data model

<Info>
You can only create or modify the reporting data model in non-committed versions, as committed versions are read-only.
</Info>

1. Open the branch view.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/branching_view.png)
</Frame>

2. If the current project version is committed, start a new version.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/start_new_version.png)
</Frame>
<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/rep6.png)
</Frame>

3. Make sure the new work-in-progress branch is selected, then go back to the process definition page.
4. Click on the Data Model icon and navigate in the object model to the target parameters.
5. Set up the business data structure to be reported by using the "Use in reporting" flag.



- Click "Show details".
- Click "Use in reporting".

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/data_to_be_reported.png)
</Frame>

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/flag_use_in_reporting.gif)
</Frame>

You can do this also for each parameter individually.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/parameter_type.png)
</Frame>


<Info>
There are three parameter structures that you can report:

**Singleton** or primitive, for which a single value is saved for each process instance. They can be of the types number, string or Boolean and will all be found in the reporting table named `params_{process_alias}`.

**Array of primitives**, in which several rows are saved for each process instance, in a dedicated table (one value per row).

**Array of objects**, in which the system extracts one or more “leaves” from each object of an array of objects. Also saved as a dedicated table.
</Info>

### Primitive parameters reporting

In the following example, there are 3 simple parameters that will be extracted: `loan_approvalFee`, `loan_currency` and `loan_downPayment`. 
They will be reported in the table `params_{process_alias}`, one row per instance.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/primitive_parameters_reporting.png)
</Frame>
### Arrays of primitives reporting

In the following example, the applicant can submit more than one email address, so each email will be extracted to a separate row in a dedicated table in the reporting database.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/arrays_of_primitives.png)
</Frame>

<Warning>
Extracting arrays is computationally intensive, do not use them if the only purpose is just to aggregate them afterward. Aggregation should be performed in the process, not in the reporting stage.
</Warning>

### Array of objects reporting

In this example, the applicant can have several real estate assets, so a subset of data items (`currentValue`, `mortgageBalance`, `propertyType`) will be extracted for each one of them.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/470/array_of_objects.png)
</Frame>

<Warning>
Extracting arrays of objects is even more is computationally intensive, do not use them if the only purpose is just to aggregate them afterward. Aggregation should be performed in the process, not in the reporting stage.
</Warning>

{/*
## Reporting plugin data model best practices

<AccordionGroup>
<Accordion title="Defining and extracting a Data Model for reporting">
In FlowX projects, every process is backed by a large, potentially complex JSON structure containing all the variables declared and used at runtime. To enable effective analytics, **the reporting plugin needs to extract only the data elements necessary for operational and business analysis**, resulting in a significantly smaller and flatter JSON-type data model. This extracted model ignores the evolving implementation details of the full process JSON; instead, it focuses on stable, business-relevant fields. The reporting plugin then uses these essential data points to populate a relational database, ensuring that only the required information is captured—providing a clear, consistent basis for reporting and analytics, both for finished and in-progress processes.
</Accordion>

<Accordion title="The process JSON vs. the smaller reporting subset">
Most of the time, the “main” JSON that drives the process contains every variable needed for execution. It can become quite large and deeply nested with details like technical parameters, intermediary states, or debug flags. Over time, as new features are added or business rules change, additional layers and properties might be introduced or removed, making this JSON evolve rapidly and often in unpredictable ways.

By contrast, for reporting purposes, you only extract a smaller, flatter subset of the data: the points actually used in business analysis. This “reporting JSON” extracts core fields (such as customer fields, transaction numbers, dates) while ignoring transient or overly technical details. It should also be fairly stable over time and not depend on the actual implementation details, so your reporting remains consistent even as the underlying process changes.

**Example of the “big” process JSON** (with nested structures and details that may change):

<Info>
The reporting fields, highlighted below, omit low-level process details like workflow steps, retries, debug logs, or nested metadata. It pulls out only the pieces of information actually needed for regular business reporting, ensuring the reporting model is concise, stable, and not prone to frequent changes.
</Info>

```json {2-5, 32-35}
{
  "order": {
    "id": 54321,
    "status": "PENDING",
    "orderDate": "2025-01-01",
    "items": [
      {
        "sku": "SKU123",
        "quantity": 2,
        "metadata": {
          "internalId": "A1B2C3",
          "isBackordered": false
        }
      },
      {
        "sku": "SKU124",
        "quantity": 1,
        "metadata": {
          "internalId": "A1B2C3",
          "isBackordered": true
        }
      }
    ],
    "workflowState": {
      "step": 3,
      "retries": 2,
      "debugInfo": {
        "lastError": "timeout"
      }
    }
  },
  "customer": {
    "name": "Jane Doe",
    "contact": {
      "email": "jane.doe@example.com",
      "phone": "+1-202-555-0173"
    },
    "preferences": {
      "newsletter": true,
      "smsOptIn": false
    }
  },
  "technicalLogs": {
    "startTime": "2025-01-01T09:00:00Z",
    "executionTimeMs": 1203,
    "sourceSystem": "v2.1"
  }
}



```
</Accordion>



<Accordion title="Setting up the Process Reported Parameters from the Start and sticking to it">   

One of the key early steps in designing your automation is meeting with business stakeholders to establish which data points truly matter for reporting. By agreeing together on a **precise set of metrics**—such as order ids, critical dates, or customer details—and “freezing” them in both name, JSON path and definition, you create a stable reporting baseline. While adding new elements is possible over time, the original core set remains intact. This approach decouples the business view of your data from the underlying technical or process-level details that may change with new features or system updates. As a result, it simplifies reporting because the agreed-upon parameters remain consistent and predictable, no matter how often the internal structure of the process JSON evolves.

To be more precise, if the JSON “path” of a reporting data point (ex. `customer.contact.email`) changes in the most current build (considered the “master version” and used for restating all historical process instances), the actual customer email might not appear for some historical instances.

NOTE: if only the name of the property changes but not the path (ex. `customer.contact.email_address`), the reporting plugin will use an internal ID number to extract the correct data from historical instances. However if the path or type (ex. from string to object or to array) of a parameter changes, it will probably not be retrieved from historical version instances.

This is why it is very important to agree on a set of business parameters to be extracted and keep them in the same JSON position as the process evolves over time.

</Accordion>


<Accordion title="Array Limitations of the Reporting Data Model">

Within the reporting data model, you can nest objects to practically any depth, and you may also include arrays of objects when necessary. However, arrays of arrays are not supported. In other words, while you can store an array of line items or child entities, you cannot place another array inside that array. This limitation simplifies processing and ensures consistency in how the data is read and stored. Although deeper nesting is technically possible, keeping your reporting model as flat as feasible still avoids unnecessary complexity, making it easier to maintain and analyze the data.

**Supported: Arrays of Objects**

```json
{
  "orderId": 12345,
  "items": [
    {
      "sku": "SKU001",
      "quantity": 2
    },
    {
      "sku": "SKU002",
      "quantity": 1
    }
  ]
}
```

Here, `items` is an array containing two individual objects, each representing a single line item. You can readily extract data (e.g., `sku`, `quantity`) for reporting.

**Not Supported: Arrays of Arrays**

```json
{
  "orderId": 67890,
  "itemBatches": [
    [
      {
        "sku": "SKU111",
        "quantity": 2
      },
      {
        "sku": "SKU222",
        "quantity": 5
      }
    ],
    [
      {
        "sku": "SKU333",
        "quantity": 1
      }
    ]
  ]
}
```

Here, `itemBatches` is an array containing other arrays, each of which holds objects. This multi-level nesting complicates the data extraction process and is not supported in the reporting data model. Instead, keep lists in a single, flat array whenever possible, even if multiple “batches” exist, to ensure the data remains easy to process and consistent for reporting.

</Accordion>
</AccordionGroup>
*/}

## Enabling process reporting

Enable reporting by checking the “Use process in reporting” button on the settings/general page of each process.

- Modifying the data model of a process version has no impact until the moment the version or build of the project is set in the "Active policy" tab and is also set to "use_in_reporting".

The reporting refresh schedule is set for a fixed interval (currently 5 minutes):

- No processing overlaps are allowed. If processing takes more than 5 minutes, the next processing is automatically postponed until the current one is finished.
- The number of Spark executors is automatically set by the reporting plugin depending on the volume of data to be processed, up to a certain fixed limit that is established in the cloud environment. This limit can be adapted depending on your needs.
- Rebuilding the whole history for a process (if the master data model, process name or the Published version change) typically takes more time. It is better to make these changes after the working hours.

## Reporting database

### Main tables

Common fields for joins:

- `inst_id`: Unique identifier for each process instance
- `query_time`: Timestamp for when the plugin extracts data


<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/rep20.png)
</Frame>

**Useful fields from instances table**:

- `date_started/finished` timestamps
- `process_alias` (process name with "_" instead of spaces)

<Info>
The published version alias will be used for all the reported versions.
</Info>

- State of the process (started, finished, etc.)
- `context_name`: Useful if the process is started by another process

**Useful fields from current_nodes table**:

- `node_started/finished` timestamps for the nodes
- `node_name`, `node_type`
- `swimlane` of the current node
- `prev_node_end`: For calculating when a token is waiting for another process branch

**Useful fields from token_history table**:

Similar to `current_nodes` but includes all nodes in the instance history.

### Parameters and object tables

Common fields, on which the joins are built:

- `inst_id`, unique identifier for each process instance
- `query_time`, recorded at the moment the plugin extracts data from the database

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/release40/reporting30.png)
</Frame>

For the `params_` table, there is a single row per process instance. For arrays and arrays of objects tables, there are multiple rows per process instance.

## Using Superset for data exploration

### Data sources

The **Data** tab represents the sources of all information:

* Databases
* CSV files
* Tables
* Excel files

Reporting plugin can be used with Superset by connecting it with a PostgreSQL DB.

### Charts

Charts represent the output of the information. There are multiple visualization charts/ plugins available.

### Dashboards

With the use of dashboards, you can share persuading flows, show how metrics change in various scenarios and match your company efforts with logical, evidence‐based visual indicators.

### Datasets

Contains all the information for extracting and processing data from the DB, including SQL queries, calculated metrics information, cache settings, etc. Datasets can also be exported / imported. 

### Connecting to a database

Before using Superset, ensure you have a PostgreSQL database installed and configured. Follow these guides for setup:

[FlowX Engine DB configuration](../../../../../setup-guides/flowx-engine-setup-guide/engine-setup)

[Reporting DB configuration](../../../../../setup-guides/plugins-setup-guide/reporting-setup)

<Info>
Read-only users should be used in production in the reporting-plugin cronjob.
</Info>

To connect Superset to a database, follow the next steps:

1. From the Toolbar, hover your cursor over the **"+"** icon, then select **Data**, and then select **Connect Database**.

<Frame>   
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/connecting_to_db.png)
</Frame>

2. The **Connect a database** window appears. Select the appropriate **database card** (in this case - PostgreSQL).

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/connect_db_superset.png)
</Frame>

3. After you selected the DB, click **Connect this database with a SQLAlchemy URI string instead?**.
4. Fill in the **SQLALCHEMY URI** and then click **Connect**.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/superset_db_URI.png)
</Frame>

<Info>
The **SQLAlchemy URI** for **reporting-db** should be in this format: `postgresql://postgres:XXXXXXXXXX@reporting-plugin-postgresql:{{port}}/reporting`.
</Info>


### Creating and configuring charts

There are multiple ways in which you can create/configure a chart.

#### Creating charts using Datasets tab

To create a Chart using the first method, you must follow the next steps:

1. From the top toolbar, select **Data** and then **Datasets**.

<Check>
You need to have a dataset added to Superset first. From that particular dataset you can build a visualization.
</Check>

2. Select the desired **dataset**.
3. On the **explore** page, choose the **visualization type** and click **Create chart**.


<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/superset_add_chart.gif)
</Frame>

4. When you select a dataset, by default **table** visualization type is selected.

<Info>
To view all the existent visualization types, click **View all charts**, the charts' gallery will open.
</Info>

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/superset_visualization_type.png)
</Frame>

#### Creating charts using Chart gallery

Using the Chart gallery is a useful method when you are not quite sure about which chart will be best for you.

To create a Chart using the second method, you must follow the next steps:

1. Select the **"+"** icon from the top toolbar and choose **Chart**.
2. Choose the **dataset** and **chart type**.
3. Review the description and example graphics of the selected chart, then click **Create new chart**.

<Tip>
If you wish to explore all the chart types available, filter by **All charts**. The charts are also grouped by categories.
</Tip>

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/superset_add_chart_second.gif)
</Frame>

### Configuring a Chart

Configure the **Query** fields by dragging and dropping relevant columns to the matching fields. Note that time and query form attributes vary by chart type.

### Exporting/importing a Chart

You can export and import charts to help you analyze your data and manipulate dashboards. To export/import a chart, follow the next steps:

1. Open **Superset** and navigate to **Charts** from the top navigation bar.
2. Select the desired **chart** and click the **breadcrumbs** menu in the top-right corner.
3. Choose an export option: .CSV, .JSON, or Download as image.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/reporting_export_imp.png)
</Frame>

**Table example**:

* **Time** - time related form attributes
* **Query** - query attributes
  * **Dimensions** - one or many columns to group by
  * **Metrics** - metrics to display
  * **Percentage metrics** - metrics for which percentage of total are to be displayed, calculated from only data within the row limit
  * **Filters** - metric used for filtering
  * **Sort by** - metric used to define how the top series are sorted if a series or row limit is present
  * **Row limit** - limits the number of rows that get displayed

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/superset_query.png)
</Frame>

### Creating a dashboard

To create a dashboard follow the next steps:

1. Create a new chart and save it to a new dashboard.
2. To publish, click **Save and go to Dashboard**.

<Frame>
![](https://s3.eu-west-1.amazonaws.com/docx.flowx.ai/platform-deep-dive/save_dashboard.gif)
</Frame>


For details on how to configure the FlowX.AI reporting plugin, check the following section:

<Card title="Reporting setup guide" href="../../../../../setup-guides/plugins-setup-guide/reporting-setup" icon="gear"/>

