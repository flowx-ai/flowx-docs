---
title: FlowX Admin setup
description: The FlowX.AI Admin microservice manages process-related entities and provides the REST API used by the FlowX.AI Designer. The processes defined here will be handled by the FlowX.AI Engine. The Admin microservice uses most of the same resources as the FlowX.AI Engine.
icon: terminal
---


## Infrastructure prerequisites

Before setting up the Admin microservice, ensure the following components are properly set up:

- **Database Instance**: The Admin microservice connects to the same database as the FlowX.AI Engine.


## Dependencies

Ensure the following dependencies are met:

- **[Database](#database-configuration)**: Properly configured database instance.
- **[Datasource](#datasource-configuration)**: Configuration details for connecting to the database where also the FlowX.Ai Engine is connected.
- **Kafka cluster**: If you intend to use the [**FlowX.AI Audit**](../../4.0/docs/platform-deep-dive/core-extensions/audit) functionality, ensure that the backend microservice can connect to the Kafka cluster. When connected to Kafka, it sends details about all database transactions to a configured Kafka topic.

## Datasource configuration

To store process definitions the Admin microservice connects to the same Postgres / Oracle database as the Engine. Make sure to set the needed database connection details.

The following configuration details need to be added using environment variables:

* `SPRING_DATASOURCE_URL`: This environment variable is used to specify the URL of the database that the Admin microservice and Engine connect to. The URL typically includes the necessary information to connect to the database server, such as the host, port, and database name. It follows the format of the database's JDBC URL, which is specific to the type of database being used (e.g., PostgreSQL or Oracle).
* `SPRING_DATASOURCE_USERNAME`: This environment variable sets the username that the Admin microservice and Engine used to authenticate themselves when connecting to the database. The username is used to identify the user account that has access to the specified database.
* `SPRING_DATASOURCE_PASSWORD`: This environment variable specifies the password associated with the username provided in the `SPRING_DATASOURCE_USERNAME` variable. The password is used to authenticate the user and grant access to the database.

<Warning>
You will need to make sure that the user, password, connection link and db name are configured correctly, otherwise, you will receive errors at start time.
</Warning>

<Info>
The database schema is managed by a [liquibase](https://www.liquibase.org/) script provided with the Engine.
</Info>


### MongoDB configuration

The Admin microservice also connects to a MongoDB database instance for additional data management. Configure the MongoDB connection with the following environment variables:

* `SPRING_DATA_MONGODB_URI` - URI for connecting to the Admin MongoDB instance
    * Format: `mongodb://${DB_USERNAME}:${DB_PASSWORD}@<host1>,<host2>,<arbiter-host>:<port>/${DB_NAME}?retryWrites=false`
* `DB_USERNAME`: `admin`.
* `DB_PASSWORD`: DB password.
* `DB_NAME`: `admin`.
* `SPRING_DATA_MONGODB_STORAGE` - Specifies the storage type used for the Runtime MongoDB instance (Azure environments only) 
  * **Possible Values:** `mongodb`, `cosmosdb`
  * **Default Value:** `mongodb`

<Info>
Ensure that the MongoDB configuration is compatible with the same database requirements as the FlowX.AI Engine, especially if sharing database instances.
</Info>


## Kafka configuration

The Admin microservice uses Kafka for sending audit logs, managing scheduled timer events, platform component versions, and start timer event updates. Both producers and consumers must be configured to ensure proper communication between services.

### General Kafka configuration

| Environment Variable              | Description             | Example Value     | Default Value    |
| --------------------------------- | ----------------------- | ----------------- | ---------------- |
| `SPRING_KAFKA_BOOTSTRAP_SERVERS`  | Kafka broker addresses  | `localhost:9092`  | `localhost:9092` |
| `SPRING_KAFKA_SECURITY_PROTOCOL`  | Security protocol       | `PLAINTEXT`       | `PLAINTEXT`      |
| `KAFKA_MESSAGE_MAX_BYTES`         | Maximum message size    | `52428800` (50MB) | `52428800`       |

### Kafka producer configuration

| Environment Variable                     | Description            | Example Value                                            |
| ---------------------------------------- | ---------------------- | -------------------------------------------------------- |
| `SPRING_KAFKA_PRODUCER_KEY_SERIALIZER`   | Key serializer class   | `org.apache.kafka.common.serialization.StringSerializer` |
| `SPRING_KAFKA_PRODUCER_MAX_REQUEST_SIZE` | Maximum request size   | `52428800` (50MB)                                        |

### Kafka consumer configuration

| Environment Variable                          | Description                      | Default Value            |
| --------------------------------------------- | -------------------------------- | ------------------------ |
| `KAFKA_CONSUMER_GROUP_ID_GENERIC_PROCESSING`  | Generic processing consumer group| `genericProcessingGroup` |
| `KAFKA_CONSUMER_THREADS_GENERIC_PROCESSING`   | Generic processing threads       | `6`                      |
| `KAFKA_AUTH_EXCEPTION_RETRY_INTERVAL`         | Auth exception retry (seconds)   | `10`                     |

### Topic naming convention and pattern creation

The Admin microservice uses a consistent naming structure for Kafka topics to ensure standardized communication.

| Environment Variable              | Description           | Default Value |
| --------------------------------- | --------------------- | ------------- |
| `KAFKA_TOPIC_NAMING_PACKAGE`      | Base package          | `ai.flowx.`   |
| `KAFKA_TOPIC_NAMING_ENVIRONMENT`  | Environment prefix    | `dev.`        |
| `KAFKA_TOPIC_NAMING_VERSION`      | Topic version         | `.v1`         |
| `KAFKA_TOPIC_NAMING_SEPARATOR`    | Primary separator     | `.`           |
| `KAFKA_TOPIC_NAMING_SEPARATOR2`   | Secondary separator   | `-`           |

Topics are constructed using the following pattern:
```
{prefix} + service + {separator/dot} + action + {separator/dot} + detail + {suffix}
```

Where:
- `prefix` is `${kafka.topic.naming.package}${kafka.topic.naming.environment}` (e.g., `ai.flowx.dev.`)
- `suffix` is `${kafka.topic.naming.version}` (e.g., `.v1`)

### Kafka topic configuration

#### Audit topics

| Environment Variable     | Description              | Pattern                                                                         | Example Value                        |
| ------------------------ | ------------------------ | ------------------------------------------------------------------------------- | ------------------------------------ |
| `KAFKA_TOPIC_AUDIT_OUT`  | Audit output topic       | `${kafka.topic.naming.prefix}core${dot}trigger${dot}save${dot}audit${kafka.topic.naming.suffix}` | `ai.flowx.dev.core.trigger.save.audit.v1` |

#### Platform topics

| Environment Variable                          | Description                      | Pattern                                                                                                  | Example Value                                        |
| --------------------------------------------- | -------------------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| `KAFKA_TOPIC_PLATFORM_COMPONENTS_VERSIONS_IN` | Components versions caching topic| `${kafka.topic.naming.prefix}core${dot}trigger${dot}platform${dot}versions${dot}caching${kafka.topic.naming.suffix}` | `ai.flowx.dev.core.trigger.platform.versions.caching.v1` |

#### Events Gateway topics

| Environment Variable                   | Description                | Pattern                                                                                           | Example Value                                         |
| -------------------------------------- | -------------------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| `KAFKA_TOPIC_EVENTS_GATEWAY_OUT_MESSAGE` | Commands message output topic | `${kafka.topic.naming.prefix}eventsgateway${dot}process${dot}commands${dot}message${kafka.topic.naming.suffix}` | `ai.flowx.dev.eventsgateway.process.commands.message.v1` |

#### Build topics

| Environment Variable                           | Description                | Pattern                                                                                                  | Example Value                                         |
| ---------------------------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| `KAFKA_TOPIC_BUILD_START_TIMER_EVENTS_OUT_UPDATES` | Start timer events updates topic | `${kafka.topic.naming.prefix}build${dot}start${dash}timer${dash}events${dot}updates${dot}in${kafka.topic.naming.suffix}` | `ai.flowx.dev.build.start-timer-events.updates.in.v1` |

#### Resource topics

| Environment Variable                      | Description                 | Pattern                                                                                                 | Example Value                                           |
| ----------------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| `KAFKA_TOPIC_RESOURCES_USAGES_REFRESH`    | Resources usages refresh topic | `${kafka.topic.naming.prefix}application${dash}version${dot}resources${dash}usages${dot}refresh${kafka.topic.naming.suffix}` | `ai.flowx.dev.application-version.resources-usages.refresh.v1` |

### OAuth authentication (when using SASL_PLAINTEXT)

When using OAuth authentication with Kafka (SASL_PLAINTEXT), activate the `kafka-auth` profile and configure the following environment variables:

| Environment Variable             | Description           | Example Value       |
| -------------------------------- | --------------------- | ------------------- |
| `KAFKA_OAUTH_CLIENT_ID`          | OAuth client ID       | `kafka`             |
| `KAFKA_OAUTH_CLIENT_SECRET`      | OAuth client secret   | `kafka-secret`      |
| `KAFKA_OAUTH_TOKEN_ENDPOINT_URI` | OAuth token endpoint  | `kafka.auth.localhost` |

<Info>
When using the `kafka-auth` profile, the security protocol will be set to `SASL_PLAINTEXT` and the appropriate SASL mechanism and configuration will be applied automatically.
</Info>

## Redis configuration

The following values should be set with the corresponding Redis-related values:

* `SPRING_REDIS_HOST`
* `SPRING_REDIS_PASSWORD`

## Logging

The following environment variables could be set in order to control log levels:

* `LOGGING_LEVEL_ROOT` - root Spring Boot microservice logs
* `LOGGING_LEVEL_APP` - app level logs

## Authorization & access roles

The following variables need to be set in order to connect to the identity management platform:

* `SECURITY_OAUTH2_BASE_SERVER_URL`
* `SECURITY_OAUTH2_CLIENT_CLIENT_ID`
* `SECURITY_OAUTH2_REALM`

A specific service account should be configured in the OpenID provider to allow the Admin microservice to access realm-specific data. It can be configured using the following environment variables:

* `SECURITY_OAUTH2_SERVICE_ACCOUNT_ADMIN_CLIENT_ID` - the openid service account username
* `SECURITY_OAUTH2_SERVICE_ACCOUNT_ADMIN_CLIENT_SECRET` - the openid service account client secret

Configuration needed to clear the offline sessions of a user session from the identity provider solution:

* `FLOWX_AUTHENTICATE_CLIENTID` 

<Card title="Configuring access rights for admin" href="./access-management/configuring-access-rights-for-admin">
</Card>

## Elasticsearch

* `SPRING_ELASTICSEARCH_REST_URIS`
* `SPRING_ELASTICSEARCH_REST_DISABLESSL`
* `SPRING_ELASTICSEARCH_INDEX_SETTINGS_NAME` 
* `SPRING_ELASTICSEARCH_REST_USERNAME`
* `SPRING_ELASTICSEARCH_REST_PASSWORD`


## Undo/redo actions

```yaml
flowx:
  undo-redo:
    ttl: 6000000  # Redis TTL for undoable actions by user+nodeid (in seconds)
    cleanup:
      cronExpression: "0 2 * * * *"  # Every day at 2am
      days: 2  # Items marked as deleted will be permanently removed if older than this number of days
```
