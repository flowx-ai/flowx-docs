---
title: Integration designer setup
description: This guide provides step-by-step instructions to set up and configure the Integration Designer service, including database, Kafka, and OAuth2 authentication settings, to ensure integration and data flow management
icon: arrow-progress
---

## Infrastructure Prerequisites

The Integration Designer service requires the following components to be set up before it can be started:

* **Docker Engine** - version 17.06 or higher
* **PostgreSQL** - version 13 or higher for managing advancing data source
* **MongoDB** - version 4.4 or higher for managing integration and runtime data
* **Kafka** - version 2.8 or higher for event-driven communication between services
* **OAuth2 Authentication** - Ensure a Keycloak server or compatible OAuth2 authorization server is configured

## Dependencies

Integration Designer is built as a Docker image and relies on several backend services and components. Ensure these services are set up and running before starting Integration Designer.

* [**Database configuration**](#database-configuration)
* [**Kafka configuration**](#configuring-kafka)
* [**Authentication & access roles**](#configuring-authentication-and-access-roles)
* [**Logging**](./setup-guides-overview#logging)

## Configuration

### Database Configuration

Integration Designer uses both PostgreSQL and MongoDB for managing advancing data and integration information. Configure these database connections with the following environment variables:

#### PostgreSQL (Advancing Data Source)

* `ADVANCING_DATASOURCE_URL` - Database URL for the advancing data source in PostgreSQL  
* `ADVANCING_DATASOURCE_USERNAME` - Username for the advancing data source in PostgreSQL


#### MongoDB (Integration Data and Runtime Data)

* `SPRING_DATA_MONGODB_URI` - URI for connecting to MongoDB for integration data  
* `DB_USERNAME` - Username for connecting to the MongoDB integration database
* `SPRING_DATA_MONGODB_RUNTIME_URI` - URI for connecting to MongoDB for runtime data  
* `RUNTIME_DB_USERNAME` - Username for connecting to the MongoDB runtime database
* `SPRING_DATA_MONGODB_STORAGE` - Specifies the storage type used for MongoDB 
    * **Possible values**: `mongodb`, `cosmosdb`


### Configuring Kafka

To configure Kafka for Integration Designer, set the following environment variables. This configuration includes naming patterns, consumer group settings, and retry intervals for authentication exceptions.

#### General Kafka Configuration

* `SPRING_KAFKA_BOOTSTRAP_SERVERS` - Address of the Kafka server in the format `host:port`
* `KAFKA_TOPIC_NAMING_ENVIRONMENT` - Environment-specific suffix for Kafka topics

#### Kafka Consumer Settings

* `KAFKA_CONSUMER_GROUP_ID_START_WORKFLOWS` - Consumer group ID for starting workflows  
  * **Default Value:** `start-workflows-group`

* `KAFKA_CONSUMER_THREADS_START_WORKFLOWS` - Number of Kafka consumer threads for starting workflows  
  * **Default Value:** `3`

* `KAFKA_AUTH_EXCEPTION_RETRY_INTERVAL` - Interval (in seconds) between retries after an `AuthorizationException`  
  * **Default Value:** `10`


#### Kafka Topic Naming Structure

The Kafka topics for Integration Designer use a structured naming convention with dynamic components, allowing for easy integration across environments. This setup defines separators, environment identifiers, and specific naming patterns for both engine and integration-related messages.

##### Naming Components

* `dot` - Primary separator for Kafka topics  
  * **Value:** `"."`
* `dash` - Secondary separator for Kafka topics  
  * **Value:** `"-"`

##### Naming Patterns

* **Package** - `kafka.topic.naming.package`  
  * **Pattern:** `ai${dot}flowx${dot}`
  * **Example Value:** `ai.flowx.`

* **Environment** - `kafka.topic.naming.environment`  
  * **Pattern:** `dev${dot}`
  * **Example Value:** `dev.`

* **Version** - `kafka.topic.naming.version`  
  * **Pattern:** `${dot}v1`
  * **Example Value:** `.v1`

* **Prefix** - Combines `package` and `environment` to create a consistent topic prefix  
  * **Pattern:** `${kafka.topic.naming.package}${kafka.topic.naming.environment}`
  * **Example Value:** `ai.flowx.dev.`

* **Suffix** - Appends version information to the end of topic names  
  * **Pattern:** `${kafka.topic.naming.version}`
  * **Example Value:** `.v1`

##### Predefined Patterns for Services

* **Engine Receive Pattern** - `kafka.topic.naming.engineReceivePattern`  
  * **Pattern:** `engine${dot}receive${dot}`
  * **Example Topic Prefix:** `ai.flowx.dev.engine.receive.`

* **Integration Receive Pattern** - `kafka.topic.naming.integrationReceivePattern`  
  * **Pattern:** `integration${dot}receive${dot}`
  * **Example Topic Prefix:** `ai.flowx.dev.integration.receive.`

#### Kafka Topics

* **Events Gateway - Outgoing Messages**  
  * **Topic:** `${kafka.topic.naming.prefix}eventsgateway${dot}receive${dot}workflowinstances${kafka.topic.naming.suffix}`
  * **Purpose:** Topic for outgoing workflow instance messages from the events gateway
  * **Example Value:** `ai.flowx.dev.eventsgateway.receive.workflowinstances.v1`

* **Engine Pattern**  
  * **Pattern:** `${kafka.topic.naming.prefix}${kafka.topic.naming.engineReceivePattern}`
  * **Purpose:** Topic pattern for receiving messages by the engine service
  * **Example Value:** `ai.flowx.dev.engine.receive.*`

* **Integration Pattern**  
  * **Pattern:** `${kafka.topic.naming.prefix}${kafka.topic.naming.integrationReceivePattern}*`
  * **Purpose:** Topic pattern for receiving messages by the integration service
  * **Example Value:** `ai.flowx.dev.integration.receive.*`

> **Note:** Replace placeholders with appropriate values for your environment before starting the service.

#### Kafka OAuth Authentication

Configure OAuth credentials for secure Kafka communication:

* `KAFKA_OAUTH_CLIENT_ID` - OAuth Client ID for Kafka
* `KAFKA_OAUTH_CLIENT_SECRET` - OAuth Client Secret for Kafka
* `KAFKA_OAUTH_TOKEN_ENDPOINT_URI` - OAuth Token Endpoint URI for obtaining Kafka tokens  
  * **Format:** `https://<auth-server>/auth/realms/<realm>/protocol/openid-connect/token`




### Configuring Health and Management Endpoints

Integration Designer provides various management endpoints for health monitoring. Configure health and readiness checks with the following variables:

* `MANAGEMENT_HEALTH_KAFKA_ENABLED` - Enables Kafka health checks (default: `true`)
* `MANAGEMENT_ENDPOINT_HEALTH_GROUP_READINESS_INCLUDE` - Defines checks included for readiness
* `MANAGEMENT_ENDPOINT_HEALTH_GROUP_LIVENESS_INCLUDE` - Defines checks included for liveness

### Configuring Authentication and Access Roles

Integration Designer uses OAuth2 for secure access control. Set up OAuth2 configurations with these environment variables:

#### Authentication and Access Roles

* `SECURITY_OAUTH2_BASE_SERVER_URL` - Base URL for the OAuth2 authorization server  
* `SECURITY_OAUTH2_REALM` - Realm for OAuth2 authentication  
* `SECURITY_OAUTH2_CLIENT_CLIENT_ID` - Client ID for the Integration Designer OAuth2 client  
* `SECURITY_OAUTH2_CLIENT_CLIENT_SECRET` - Client Secret for the Integration Designer OAuth2 client  
* `SECURITY_OAUTH2_SERVICE_ACCOUNT_ADMIN_CLIENT_ID` - Client ID for the Keycloak admin service account  
* `SECURITY_OAUTH2_SERVICE_ACCOUNT_ADMIN_CLIENT_SECRET` - Client Secret for the Keycloak admin service account  

### Configuring Logging

To control the log levels for Integration Designer, set the following environment variables:

* `LOGGING_LEVEL_ROOT` - The log level for root Spring Boot microservice logs
* `LOGGING_LEVEL_APP` - The log level for application-level logs
